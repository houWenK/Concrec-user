{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5449d9d2",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182fe8e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb669082",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_item_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'all_rating_min_max',\n",
    "        'members_min_max',\n",
    "        'aired_from_min_max',\n",
    "        'aired_to_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()\n",
    "\n",
    "def get_user_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'user_rating_ave_min_max',\n",
    "        'user_rating_std_min_max',\n",
    "        'user_aired_from_ave_min_max',\n",
    "        'user_aired_to_ave_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6120aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multihot_feature(df, feat_name):\n",
    "    feat_df = df[[feat_name]]\n",
    "    feat_vecs = feat_df.to_numpy()\n",
    "    feat_vec = np.apply_along_axis(lambda v: v[0], 1, feat_vecs)\n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4711be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    label_df = df[['label']]\n",
    "    return label_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec525757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(df):\n",
    "    return ( \n",
    "        get_multihot_feature(df, 'genres_multihot'),\n",
    "        get_multihot_feature(df, 'user_liked_genres_multihot'),\n",
    "        get_item_numeric_features(df),\n",
    "        get_user_numeric_features(df) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8c0cc",
   "metadata": {},
   "source": [
    "## Load Parguet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d00494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905afa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../../data/anime/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724246b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47d85ca",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ab2ef",
   "metadata": {},
   "source": [
    "x1: item categorical feature        \n",
    "x2: user categorical feature          \n",
    "x3: item numeric features          \n",
    "x4: user numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8d15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74bf2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7854f2",
   "metadata": {},
   "source": [
    "### HParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6ed8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "# HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "# HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_LAYERS,\n",
    "    HP_LAYER_SIZE,\n",
    "    HP_LEARN_RATE\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a101f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x1_shape, x2_shape, x3_shape, x4_shape, hparams):\n",
    "    x1_input = keras.layers.Input(shape=(x1_shape,))\n",
    "    x2_input = keras.layers.Input(shape=(x2_shape,))\n",
    "    x3_input = keras.layers.Input(shape=(x3_shape,))\n",
    "    x4_input = keras.layers.Input(shape=(x4_shape,))\n",
    "    \n",
    "    # compact embedding for x1 and x2\n",
    "    compact_x1 = keras.layers.Dense(10)(x1_input)\n",
    "    compact_x2 = keras.layers.Dense(10)(x2_input)\n",
    "    \n",
    "    # concat all\n",
    "    merge = keras.layers.concatenate([compact_x1, compact_x2, x3_input, x4_input])\n",
    "    \n",
    "    # hidden layers\n",
    "    h_input = merge\n",
    "    for _ in range(hparams[HP_LAYERS]):\n",
    "        h = keras.layers.Dense(hparams[HP_LAYER_SIZE], activation='relu')(h_input)\n",
    "        h_input = h\n",
    "    \n",
    "    # output\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(h_input)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[x1_input, x2_input, x3_input, x4_input], outputs=output)\n",
    "    \n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d4c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd51dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b002c6",
   "metadata": {},
   "source": [
    "## Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa1d8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1s = []\n",
    "test_x2s = []\n",
    "test_x3s = []\n",
    "test_x4s = []\n",
    "test_ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3602f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../../data/anime/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames\n",
    "\n",
    "filenames = data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1bed625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_id, hparams):\n",
    "    # build model\n",
    "    model = build_model(43, 43, 4, 4, hparams)\n",
    "    print(f\"model id: {model_id}:\")\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    # config hparam logs\n",
    "    log_filename = f\"{model_id}\"\n",
    "    for h in hparams:\n",
    "        log_filename += f\"_{h.name}-{hparams[h]}\"\n",
    "    \n",
    "    log_dir = os.path.join(\"hparams\", log_filename)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir,\n",
    "        update_freq = 10,\n",
    "        profile_batch = 0\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "    # train model\n",
    "    for filename in filenames[:1]:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "        # shuffle and split train and test\n",
    "        train_df = df\n",
    "#         train_df = df.sample(frac=0.8, random_state=666)\n",
    "#         test_df = df.drop(train_df.index)\n",
    "\n",
    "        # get features\n",
    "        train_x1, train_x2, train_x3, train_x4 = get_all_features(train_df)\n",
    "#         val_x1, val_x2, val_x3, val_x4 = get_all_features(test_df)\n",
    "\n",
    "        # get label\n",
    "        train_y = get_label(train_df)\n",
    "#         val_y = get_label(test_df)\n",
    "\n",
    "        print('training on new dataset')\n",
    "\n",
    "        model.fit(\n",
    "            [train_x1, train_x2, train_x3, train_x4], \n",
    "            train_y, \n",
    "            validation_split=0.2,\n",
    "            batch_size=16, \n",
    "            epochs=4,\n",
    "            callbacks=[tensorboard_callback, hparams_callback]\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60cc3dd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_params():\n",
    "    with tf.summary.create_file_writer('hparams').as_default():\n",
    "            hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "            \n",
    "    model_id = 0\n",
    "    for layers in range(HP_LAYERS.domain.min_value, HP_LAYERS.domain.max_value + 1):\n",
    "        for size in HP_LAYER_SIZE.domain.values:\n",
    "            for rate in HP_LEARN_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_LAYERS: layers,\n",
    "                    HP_LAYER_SIZE: size,\n",
    "                    HP_LEARN_RATE: rate\n",
    "                }\n",
    "\n",
    "                run_model(model_id, hparams)\n",
    "                model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbcd072b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model id: 0:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5835 - accuracy: 0.6908 - val_loss: 0.5584 - val_accuracy: 0.7212\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5324 - accuracy: 0.7344 - val_loss: 0.5495 - val_accuracy: 0.7275\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5186 - accuracy: 0.7439 - val_loss: 0.5387 - val_accuracy: 0.7361\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5126 - accuracy: 0.7450 - val_loss: 0.5839 - val_accuracy: 0.6926\n",
      "model id: 1:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5749 - accuracy: 0.6981 - val_loss: 0.5689 - val_accuracy: 0.6993\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5332 - accuracy: 0.7325 - val_loss: 0.5671 - val_accuracy: 0.7079\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5224 - accuracy: 0.7400 - val_loss: 0.5541 - val_accuracy: 0.7251\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5183 - accuracy: 0.7445 - val_loss: 0.5638 - val_accuracy: 0.7186\n",
      "model id: 2:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5936 - accuracy: 0.6833 - val_loss: 0.6154 - val_accuracy: 0.6831\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5460 - accuracy: 0.7259 - val_loss: 0.5650 - val_accuracy: 0.7145\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5336 - accuracy: 0.7327 - val_loss: 0.5518 - val_accuracy: 0.7245\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5271 - accuracy: 0.7386 - val_loss: 0.5501 - val_accuracy: 0.7227\n",
      "model id: 3:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5753 - accuracy: 0.6957 - val_loss: 0.5578 - val_accuracy: 0.7189\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5310 - accuracy: 0.7353 - val_loss: 0.5522 - val_accuracy: 0.7217\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5188 - accuracy: 0.7428 - val_loss: 0.5456 - val_accuracy: 0.7256\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5120 - accuracy: 0.7481 - val_loss: 0.5482 - val_accuracy: 0.7255\n",
      "model id: 4:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5727 - accuracy: 0.6994 - val_loss: 0.5594 - val_accuracy: 0.7109\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5325 - accuracy: 0.7346 - val_loss: 0.5559 - val_accuracy: 0.7190\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5222 - accuracy: 0.7423 - val_loss: 0.5441 - val_accuracy: 0.7319\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5193 - accuracy: 0.7428 - val_loss: 0.5419 - val_accuracy: 0.7289\n",
      "model id: 5:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5917 - accuracy: 0.6855 - val_loss: 0.5810 - val_accuracy: 0.7033\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5489 - accuracy: 0.7245 - val_loss: 0.5685 - val_accuracy: 0.7119\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5349 - accuracy: 0.7349 - val_loss: 0.5470 - val_accuracy: 0.7278\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5324 - accuracy: 0.7343 - val_loss: 0.5468 - val_accuracy: 0.7324\n",
      "model id: 6:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5673 - accuracy: 0.7038 - val_loss: 0.5626 - val_accuracy: 0.7110\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5292 - accuracy: 0.7367 - val_loss: 0.5466 - val_accuracy: 0.7297\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5167 - accuracy: 0.7430 - val_loss: 0.5410 - val_accuracy: 0.7248\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5117 - accuracy: 0.7477 - val_loss: 0.5551 - val_accuracy: 0.7198\n",
      "model id: 7:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5745 - accuracy: 0.6997 - val_loss: 0.5520 - val_accuracy: 0.7193\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5358 - accuracy: 0.7303 - val_loss: 0.5579 - val_accuracy: 0.7201\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5254 - accuracy: 0.7385 - val_loss: 0.5633 - val_accuracy: 0.7186\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5205 - accuracy: 0.7427 - val_loss: 0.5630 - val_accuracy: 0.7161\n",
      "model id: 8:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5896 - accuracy: 0.6839 - val_loss: 0.5674 - val_accuracy: 0.7087\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5447 - accuracy: 0.7261 - val_loss: 0.5711 - val_accuracy: 0.7012\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5373 - accuracy: 0.7326 - val_loss: 0.5604 - val_accuracy: 0.7163\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5324 - accuracy: 0.7354 - val_loss: 0.5458 - val_accuracy: 0.7331\n",
      "model id: 9:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5735 - accuracy: 0.6969 - val_loss: 0.5600 - val_accuracy: 0.7161\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5289 - accuracy: 0.7376 - val_loss: 0.5488 - val_accuracy: 0.7273\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5179 - accuracy: 0.7442 - val_loss: 0.5486 - val_accuracy: 0.7241\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5103 - accuracy: 0.7505 - val_loss: 0.5441 - val_accuracy: 0.7275\n",
      "model id: 10:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5727 - accuracy: 0.7014 - val_loss: 0.5541 - val_accuracy: 0.7166\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5350 - accuracy: 0.7341 - val_loss: 0.5635 - val_accuracy: 0.7099\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5267 - accuracy: 0.7387 - val_loss: 0.5653 - val_accuracy: 0.7118\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5216 - accuracy: 0.7430 - val_loss: 0.5422 - val_accuracy: 0.7241\n",
      "model id: 11:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5944 - accuracy: 0.6820 - val_loss: 0.5734 - val_accuracy: 0.7106\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5492 - accuracy: 0.7221 - val_loss: 0.5505 - val_accuracy: 0.7312\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5372 - accuracy: 0.7316 - val_loss: 0.5759 - val_accuracy: 0.6977\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5325 - accuracy: 0.7370 - val_loss: 0.5381 - val_accuracy: 0.7338\n",
      "model id: 12:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5730 - accuracy: 0.6977 - val_loss: 0.5667 - val_accuracy: 0.7068\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5325 - accuracy: 0.7349 - val_loss: 0.5420 - val_accuracy: 0.7293\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5196 - accuracy: 0.7429 - val_loss: 0.5417 - val_accuracy: 0.7294\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5121 - accuracy: 0.7491 - val_loss: 0.5425 - val_accuracy: 0.7297\n",
      "model id: 13:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5765 - accuracy: 0.6971 - val_loss: 0.5595 - val_accuracy: 0.7240\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5325 - accuracy: 0.7333 - val_loss: 0.5470 - val_accuracy: 0.7278\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5273 - accuracy: 0.7379 - val_loss: 0.5380 - val_accuracy: 0.7301\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5206 - accuracy: 0.7450 - val_loss: 0.5565 - val_accuracy: 0.7274\n",
      "model id: 14:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5998 - accuracy: 0.6755 - val_loss: 0.5765 - val_accuracy: 0.7130\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5502 - accuracy: 0.7208 - val_loss: 0.5631 - val_accuracy: 0.7193\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5358 - accuracy: 0.7341 - val_loss: 0.5484 - val_accuracy: 0.7286\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 2s 1ms/step - loss: 0.5318 - accuracy: 0.7385 - val_loss: 0.5526 - val_accuracy: 0.7205\n",
      "model id: 15:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5684 - accuracy: 0.7046 - val_loss: 0.5462 - val_accuracy: 0.7237\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5281 - accuracy: 0.7362 - val_loss: 0.5522 - val_accuracy: 0.7273\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5178 - accuracy: 0.7430 - val_loss: 0.5537 - val_accuracy: 0.7094\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5106 - accuracy: 0.7475 - val_loss: 0.5550 - val_accuracy: 0.7201\n",
      "model id: 16:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5795 - accuracy: 0.6950 - val_loss: 0.5664 - val_accuracy: 0.7103\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5349 - accuracy: 0.7354 - val_loss: 0.5871 - val_accuracy: 0.6946\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5269 - accuracy: 0.7383 - val_loss: 0.5534 - val_accuracy: 0.7282\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5216 - accuracy: 0.7429 - val_loss: 0.5482 - val_accuracy: 0.7184\n",
      "model id: 17:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1845/1845 [==============================] - 3s 2ms/step - loss: 0.6054 - accuracy: 0.6726 - val_loss: 0.6376 - val_accuracy: 0.6679\n",
      "Epoch 2/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5517 - accuracy: 0.7213 - val_loss: 0.5484 - val_accuracy: 0.7300\n",
      "Epoch 3/4\n",
      "1845/1845 [==============================] - 3s 2ms/step - loss: 0.5395 - accuracy: 0.7305 - val_loss: 0.6272 - val_accuracy: 0.6652\n",
      "Epoch 4/4\n",
      "1845/1845 [==============================] - 3s 1ms/step - loss: 0.5343 - accuracy: 0.7345 - val_loss: 0.5424 - val_accuracy: 0.7296\n"
     ]
    }
   ],
   "source": [
    "test_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e581eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 69402), started 2 days, 18:41:48 ago. (Use '!kill 69402' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-76cb0f6b60a94811\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-76cb0f6b60a94811\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e00572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac56a05",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1 = np.vstack(test_x1s)\n",
    "test_x2 = np.vstack(test_x2s)\n",
    "test_x3 = np.vstack(test_x3s)\n",
    "test_x4 = np.vstack(test_x4s)\n",
    "test_y = np.vstack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate([test_x1, test_x2, test_x3, test_x4], test_y)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddae2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "g = 0\n",
    "\n",
    "for i in range(n):\n",
    "    expect = test_y[i][0]\n",
    "    x1 = test_x1[i]\n",
    "    x2 = test_x2[i]\n",
    "    x3 = test_x3[i]\n",
    "    x4 = test_x4[i]\n",
    "    xs = [ np.array([x1]), np.array([x2]), np.array([x3]), np.array([x4]) ]\n",
    "    print('xs')\n",
    "    print(xs)\n",
    "    \n",
    "    predict = model.predict(xs)\n",
    "    predict = predict[0][0]\n",
    "    if predict > 0.5:\n",
    "        predict = 1\n",
    "    else:\n",
    "        predict = 0\n",
    "        \n",
    "    if predict == expect:\n",
    "        g += 1.0\n",
    "    \n",
    "#     print(f'Expect {expect}, predict {predict}')\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('accuracy:')\n",
    "print(g / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2afd28",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6440a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d15fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
