{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Deep Learning\n",
    "### 数据预处理：分别对数值型和类别型数据进行encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18884/1793126125.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mspark\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"concrec-rank\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"spark.driver.memory\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"11g\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\soft\\Anaconda3\\envs\\Concrec\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    227\u001B[0m                         \u001B[1;31m# This SparkContext may be an existing one.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 228\u001B[1;33m                         \u001B[0msc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    229\u001B[0m                     \u001B[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    230\u001B[0m                     \u001B[1;31m# by all sessions.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\soft\\Anaconda3\\envs\\Concrec\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    383\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 384\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    385\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\soft\\Anaconda3\\envs\\Concrec\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    144\u001B[0m         \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 146\u001B[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0m\u001B[0;32m    147\u001B[0m                           conf, jsc, profiler_cls)\n\u001B[0;32m    148\u001B[0m         \u001B[1;32mexcept\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\soft\\Anaconda3\\envs\\Concrec\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m_do_init\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    222\u001B[0m         \u001B[1;31m# data via a socket.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m         \u001B[1;31m# scala's mangled names w/ $ in them require special treatment.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_encryption_enabled\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misEncryptionEnabled\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    225\u001B[0m         \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menviron\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m             \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetPythonAuthSocketTimeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\soft\\Anaconda3\\envs\\Concrec\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1528\u001B[0m                     answer, self._gateway_client, self._fqn, name)\n\u001B[0;32m   1529\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1530\u001B[1;33m             raise Py4JError(\n\u001B[0m\u001B[0;32m   1531\u001B[0m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001B[0;32m   1532\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPy4JError\u001B[0m: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"concrec-rank\") \\\n",
    "    .config(\"spark.driver.memory\", \"11g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = spark.read.csv('../data/anime/parsed_anime.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast aired_from into int\n",
    "from pyspark.sql.types import IntegerType\n",
    "anime_df = anime_df.withColumn('aired_from', col('aired_from').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = spark.read.csv('../data/anime/rating.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid rating only\n",
    "rating_df = rating_df.filter(rating_df['rating'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge rating with anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = rating_df.join(\n",
    "    anime_df.select('anime_id', 'name', 'genre', 'type', 'episodes', \n",
    "                    'rating', 'members', 'aired_from', 'aired_to').withColumnRenamed('rating', 'all_rating'), \n",
    "    on=['anime_id'], how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把用户的打分转换成是否喜欢：大于7.5分为喜欢（1），否则为不喜欢（0）        \n",
    "可以尝试不同策略，如直接用评分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_threshold = 7.5\n",
    "\n",
    "def build_label(df):\n",
    "    return df.withColumn('label',\n",
    "                         when(col('rating') >= like_threshold, 1).otherwise(0)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = build_label(merged_df)\n",
    "labeled_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window\n",
    "这里要在df的每个row上，额外增加和用户相关的信息。比如该用户最爱的电影类型、该用户看过多少电影、平均打分是多少        \n",
    "为了防止泄露未来信息，需把所有评分按照时间顺序排序，然后用滑动窗口聚合        \n",
    "理论应该使用评分时间，但是由于没有这个数据，所以采用电影上映时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window \\\n",
    "    .partitionBy('user_id') \\\n",
    "    .orderBy('aired_from') \\\n",
    "    .rowsBetween(-100, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 帮助方法：对于某一列，在聚合的时候，如果用户不喜欢这个电影，则不聚合这个电影的信息\n",
    "likedMoviesCol = lambda cname: when(col('label') == 1, col(cname)).otherwise(lit(None))\n",
    "\n",
    "@udf(types.ArrayType(types.StringType()))\n",
    "def most_liked_genres(gen_strs):\n",
    "    \"\"\"\n",
    "    gen_strs = [\"Action, Adventure, Drama\", \"Comedy, Drama, School\"]\n",
    "    \"\"\"\n",
    "    gens = [s.split(\",\") for s in gen_strs]\n",
    "    gens = [x for l in gens for x in l] # flatten\n",
    "    gens = [s.strip() for s in gens]\n",
    "    \n",
    "    gen_set = set(gens)\n",
    "    count_occur = lambda gen, l: len([g for g in l if g == gen])\n",
    "    gen_with_occur = [(gen, count_occur(gen, gens)) for gen in gen_set]\n",
    "    gen_with_occur.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # pick 3 most liked genres\n",
    "    return [x[0] for x in gen_with_occur[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_PRECISION = 2\n",
    "\n",
    "feat_df = labeled_df \\\n",
    "    .withColumn('user_rating_cnt', count(lit(1)).over(windowSpec)) \\\n",
    "    .withColumn('user_rating_ave', mean(col('rating')).over(windowSpec)) \\\n",
    "    .withColumn('user_rating_ave', F.round(col('user_rating_ave'), NUMBER_PRECISION)) \\\n",
    "    .withColumn('user_rating_std', stddev(col('rating')).over(windowSpec)) \\\n",
    "    .withColumn('user_rating_std', F.round(col('user_rating_std'), NUMBER_PRECISION)) \\\n",
    "    .withColumn('user_aired_from_ave', mean(likedMoviesCol('aired_from')).over(windowSpec)) \\\n",
    "    .withColumn('user_aired_from_ave', F.round(col('user_aired_from_ave'), 0)) \\\n",
    "    .withColumn('user_aired_to_ave', mean(likedMoviesCol('aired_to')).over(windowSpec)) \\\n",
    "    .withColumn('user_aired_to_ave', F.round(col('user_aired_to_ave'), 0)) \\\n",
    "    .withColumn('user_liked_genres', most_liked_genres(collect_list(likedMoviesCol('genre')).over(windowSpec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.select('anime_id', 'user_id', 'rating',\n",
    "#                'user_rating_cnt', 'user_rating_ave', 'user_rating_std',\n",
    "#                'user_aired_from_ave', 'user_aired_to_ave'\n",
    "               'genre', 'user_liked_genres'\n",
    "              ).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "### 将数值型和分类型特征分布进行encode表达"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Genres: multi-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import pyspark.sql.types as types\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. parse genre to list\n",
    "@udf(returnType='array<string>')\n",
    "def genre_to_list(gen_str):\n",
    "    if gen_str is None:\n",
    "        return []\n",
    "    \n",
    "    gens = gen_str.split(\",\")\n",
    "    return [gen.strip() for gen in gens]\n",
    "\n",
    "genres_df = feat_df.withColumn('genres', genre_to_list(col('genre'))).drop('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_genres_col(index_mapping_broadcasted):\n",
    "    @udf(returnType='array<int>')\n",
    "    def encode_genres_col(genres, max_genre_index):\n",
    "        \"\"\"\n",
    "        用已经训练好的string index mapping对genres数组进行encode\n",
    "        \"\"\"\n",
    "        if genres is None:\n",
    "            genres = []\n",
    "        gen_vec = [index_mapping_broadcasted.value.get(gen) for gen in genres]\n",
    "        gen_vec = list(set(gen_vec)) # dedup\n",
    "\n",
    "        # convert genre vector to multi-hot\n",
    "        fill = np.ones(len(gen_vec), dtype=np.int32)\n",
    "        sorted_index = np.sort(gen_vec)\n",
    "        multihot_vec = SparseVector(max_genre_index + 1, sorted_index, fill)\n",
    "        return multihot_vec.toArray().astype(np.int32).tolist()\n",
    "    return encode_genres_col\n",
    "    \n",
    "\n",
    "def multi_hot_encode_genres(featdf):\n",
    "    df = featdf.withColumn('genre_item', explode(col('genres')))\n",
    "    \n",
    "    genre_string_indexer = StringIndexer(inputCol='genre_item', outputCol='genre_index')\n",
    "    indexer_model = genre_string_indexer.fit(df)\n",
    "    \n",
    "    # get mapping from string indexer\n",
    "    gens_df = spark.createDataFrame(\n",
    "        [{'genre_item': g} for g in indexer_model.labels]\n",
    "    )\n",
    "    mapping_df = indexer_model.transform(gens_df).collect()\n",
    "    mapping_dict = {row.genre_item: int(row.genre_index) for row in mapping_df}\n",
    "    max_genre_index = __builtin__.max(mapping_dict.values())\n",
    "    broadcasted = spark.sparkContext.broadcast(mapping_dict)\n",
    "    \n",
    "    encode_fn = encode_genres_col(broadcasted)\n",
    "   \n",
    "    return featdf \\\n",
    "        .withColumn( 'genres_multihot', encode_fn(col('genres'), lit(max_genre_index)) ) \\\n",
    "        .withColumn( 'user_liked_genres_multihot', encode_fn(col('user_liked_genres'), lit(max_genre_index)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_encoded_df = multi_hot_encode_genres(genres_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_encoded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# genre_encoded_df.head(5)\n",
    "# genre_encoded_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  min max scaler for numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(types.FloatType())\n",
    "def extract_float(l):\n",
    "    r = __builtin__.round(l[0], NUMBER_PRECISION)\n",
    "        \n",
    "    return float(r)\n",
    "\n",
    "\n",
    "def min_max_scale(featdf, col):\n",
    "    output_col = f\"{col}_min_max\"\n",
    "    vec_assembler = VectorAssembler(inputCols=[col], outputCol=f\"{col}_vec\", handleInvalid='keep')\n",
    "    min_max_scaler = MinMaxScaler(inputCol=f\"{col}_vec\", outputCol=output_col)\n",
    "    pipeline = Pipeline(stages=[vec_assembler, min_max_scaler])\n",
    "    \n",
    "    return pipeline \\\n",
    "        .fit(featdf) \\\n",
    "        .transform(featdf) \\\n",
    "        .drop(f\"{col}_vec\") \\\n",
    "        .withColumn(output_col, extract_float(F.col(output_col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = genre_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaled_df = min_max_scale(scaled_df, 'all_rating')\n",
    "scaled_df = min_max_scale(scaled_df, 'members')\n",
    "scaled_df = min_max_scale(scaled_df, 'aired_from')\n",
    "scaled_df = min_max_scale(scaled_df, 'aired_to')\n",
    "scaled_df = min_max_scale(scaled_df, 'user_rating_ave')\n",
    "scaled_df = min_max_scale(scaled_df, 'user_rating_std')\n",
    "scaled_df = min_max_scale(scaled_df, 'user_aired_from_ave')\n",
    "scaled_df = min_max_scale(scaled_df, 'user_aired_to_ave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_df.select('anime_id', 'user_id', \n",
    "                 'user_aired_from_ave', 'user_aired_from_ave_min_max'\n",
    "                ).show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = scaled_df.select('anime_id', 'user_id', 'label', \n",
    "                             'all_rating_min_max', 'members_min_max', \n",
    "                             'aired_from_min_max', 'aired_to_min_max',\n",
    "                             'genres_multihot',\n",
    "                             'user_rating_ave_min_max', 'user_rating_std_min_max',\n",
    "                             'user_aired_from_ave_min_max', 'user_aired_to_ave_min_max',\n",
    "                             'user_liked_genres_multihot'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_df.fillna(0) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('../data/anime/dnn_feat_eng')\n",
    "#     .format('csv').option(\"header\", \"true\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "concrec",
   "language": "python",
   "display_name": "Concrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
